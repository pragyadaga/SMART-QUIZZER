import sys
import os
import re
from win32com.client import Dispatch
import summary
import nltk
import json

def paragraphs(file, separator=None):
    # reading file para by para
    # needed to summarize a big text file
    if not callable(separator):
        def separator(line): return line == '\n'
    paragraph = []
    for line in file:
        if separator(line):
            if paragraph:
                yield ''.join(paragraph)
                paragraph = []
        else:
            paragraph.append(line)
    yield ''.join(paragraph)

def summary_para(inputfile):
    # uses summary class to find generate filename_summary.txt(summary of the pdf/word )  
    # calls summarizer code
    name, ext = inputfile.split(".");
    summaryfile = name+"_summary.txt"
    if os.path.exists(summaryfile):
        os.remove(summaryfile)
    try:
        fi = open(inputfile,'r',encoding="utf8")
        contents = fi.read()
    except Exception as e:
        fi = open(inputfile, 'r')
        contents = fi.read()
    for para in paragraphs(contents):
        try:
            summary.main(summaryfile, para)
        except Exception as e:
            pass
    fi.close()

def main(argv):
    #used for converting pdf/word to text file
    inputfile = argv[0]
    name, ext = os.path.splitext(inputfile)
    # checking file type
    # converting pdf, doc, docx to text file
    if ext == '.pdf' :
        # pdf file
        outputfile = name + '.txt'
        # pdftotext.exe converts pdf to text
        if(argv[1]=='NULL' and argv[2]=='NULL'):
            os.system("C:\Python34\pdftotext.exe \""+inputfile+"\"")
        if(argv[1]!='NULL' and argv[2]=='NULL'):
            first_page_no = argv[1]
            os.system("C:\Python34\pdftotext.exe -f "+first_page_no+" \""+inputfile+"\" \""+outputfile+"\"")
        if(argv[1]!='NULL' and argv[2]!='NULL'):
            first_page_no = argv[1]
            last_page_no = argv[2]
            os.system("C:\Python34\pdftotext.exe -f "+first_page_no+" -l "+last_page_no+" \""+inputfile+"\" \""+outputfile+"\"")
        name, ext = os.path.splitext(inputfile)
    elif ext == '.doc' or ext == '.docx':
        # word file
        wordapp = Dispatch('Word.Application')
        outputfile = name + '.txt'
        wordapp.Documents.Open(os.path.abspath(inputfile))
        wdFormatTextLineBreaks = 3
        wordapp.ActiveDocument.SaveAs(os.path.abspath(outputfile), FileFormat=wdFormatTextLineBreaks)
        wordapp.ActiveDocument.Close()
    elif ext == '.txt':
        print("Txt file")
        outputfile = inputfile;        
    else : 
        print("File type not supported")

    # summarizer code is called
    summary_para(outputfile)

def create_nouns_list(file):
    # creates a text file of nouns.
    # nouns_list.txt is used by function putting_blanks
    # read from file and do pos tagging
    final_read=file.split('.')[0]+".txt"
    c=open(final_read,"r")
    str=c.read()
    t=nltk.word_tokenize(str)   # words of the text
    tagged=nltk.pos_tag(t)      # tagged list of all words of the text

    # get a list of nouns
    nouns=[]
    for i in tagged:
        if(i[1]=="NN"):
            nouns.append(i[0])

    # list of all nouns in the text(nouns repeat)           
    nouns_set=set(nouns)
    f=open("nouns_list.txt","w")
    for noun in nouns_set:
        f.write(noun+"\n")

def create_preposition_list(file):
    # creates a text file of prepositions.
    # preposition_list.txt is used by function putting_blanks
    final_read=file.split('.')[0]+".txt"
    fp_read=open(final_read,"r")
    fp_read_file=fp_read.read()
    t=nltk.word_tokenize(fp_read_file)
    tag_words=nltk.pos_tag(t)
    preposition=[]
    for i in tag_words:
        if(i[1]=='IN'):
            res=""
            for j in i[0]:
                if((j!=' ') or (j!='\n')):
                    res+=j
            if res not in preposition:
                preposition.append(res)
    fp_write=open("preposition_list.txt","w")
    for i in preposition:
        s=i+"\n"
        fp_write.write(s)
        
def summary_putting_blanks(file): 
    # generates FITB based on nouns and prepositions approach.

    # list of nouns
    fp_list_of_nouns=open("nouns_list.txt","r")
    list_of_nouns=fp_list_of_nouns.readlines()

    # list of prepostions
    fp_preposition_list=open("preposition_list.txt","r")
    list_of_preposition=fp_preposition_list.readlines()
    list_of_preposition_=[]
    for i in list_of_preposition:
        k=i.split('\n')
        list_of_preposition_.append(k[0])

    value_list=[]
    no_of_question=0    # no of question generated by putting blanks

    # # direct txt input file 
    # final_read=file[0].split('.')[0]+".txt"
    # fp_story=open(final_read,"r")
    # line_story_=fp_story.read()
    # line_story=line_story_.split('.')
    
    # for i in list_of_nouns:
    #   for line in line_story:
    #       s=line.split(" ")
    #       k=i.split('\n')
    #       if k[0] in s:
    #           c=line.index(k[0])
    #           if(len(s)>=5 and len(s)<=15):
    #               if((c<=(len(s[0])+len(s[1])+len(s[3])+len(s[3]))) or (c>=(len(s)-(len(s[len(s)-1])+len(s[len(s)-2])+len(s[len(s)-3])+len(s[len(s)-4]))))):
    #                   word_index=0
    #                   for j in range(len(s)):
    #                       if(s[j]==k[0]):
    #                           word_index=j
    #                   if((word_index!=0) and (word_index!=(len(s)-2)) and (word_index!=(len(s)-1))):
    #                       if((s[word_index-1] in list_of_preposition_) or (s[word_index+1] in list_of_preposition_)):
    #                           new_sentence=re.sub(k[0],"_________",line)
    #                           new_sentence=new_sentence+" .\n"
    #                           dict_inner={}
    #                           dict_inner["ques"]=new_sentence.strip("\n")
    #                           dict_inner["ans"]=k[0]
    #                           value_list.append(dict_inner)
    # # in json
    # dict_outer["question"]=value_list
    # print(dict_outer)

    # for the summary file
    final_read_summary=file[0].split('.')[0]+"_summary"+".txt"
    fp_story_summary=open(final_read_summary,"r")
    line_story_summary_=fp_story_summary.read()
    line_story_summary=line_story_summary_.split('.')
    for i in list_of_nouns:
        for line in line_story_summary:
            s=line.split(" ")
            k=i.split('\n')
            if k[0] in s:
                c=line.index(k[0])
                if(len(s)>=5 and len(s)<=15):
                    if((c<=(len(s[0])+len(s[1])+len(s[3])+len(s[3]))) or (c>=(len(s)-(len(s[len(s)-1])+len(s[len(s)-2])+len(s[len(s)-3])+len(s[len(s)-4]))))):
                        word_index=0
                        for j in range(len(s)):
                            if(s[j]==k[0]):
                                word_index=j
                        if((word_index!=0) and (word_index!=(len(s)-2)) and (word_index!=(len(s)-1))):
                            if((s[word_index-1] in list_of_preposition_) or (s[word_index+1] in list_of_preposition_)):
                                new_sentence=re.sub(k[0],"_________",line)
                                new_sentence=new_sentence+" .\n"
                                dict_inner={}
                                dict_inner["ques"]=new_sentence.strip("\n")
                                dict_inner["ans"]=k[0]
                                value_list.append(dict_inner)
                                no_of_question+=1

    return (value_list, no_of_question)

def find_from_glossary(input_file,glossary_file): 
    # called in main when teacher provides glossary of keywords along with pdf/word file
    input_file=input_file.split('.')[0]+".txt"
    c=open(input_file,"r")
    g=open(glossary_file,"r",encoding="utf8")
    str=c.read()
    l=str.split('.')

    p=g.readlines()
    keywords=[]
    no_of_question = 0
    
    value_list=[]
    for i in p:
        keywords.append(i.split("\n")[0])
    for i in keywords:
        for sentence in l:
            s=sentence.split(" ")
            if i in s:
                new_sentence=re.sub(i,"_____",sentence,re.IGNORECASE)
                new_sentence=new_sentence+" .\n"
                dict_inner={}
                dict_inner["ques"]=new_sentence
                dict_inner["ans"]=i
                no_of_question +=1
                value_list.append(dict_inner)
    return (value_list, no_of_question)

def input_putting_blanks(file): 
    # generates FITB based on nouns and prepositions approach.

    # list of nouns
    fp_list_of_nouns=open("nouns_list.txt","r")
    list_of_nouns=fp_list_of_nouns.readlines()

    # list of prepostions
    fp_preposition_list=open("preposition_list.txt","r")
    list_of_preposition=fp_preposition_list.readlines()
    list_of_preposition_=[]
    for i in list_of_preposition:
        k=i.split('\n')
        list_of_preposition_.append(k[0])

    value_list=[]
    no_of_question=0    # no of question generated by putting blanks

    # direct txt input file 
    final_read=file[0].split('.')[0]+".txt"
    fp_story=open(final_read,"r")
    line_story_=fp_story.read()
    line_story=line_story_.split('.')
    
    for i in list_of_nouns:
      for line in line_story:
          s=line.split(" ")
          k=i.split('\n')
          if k[0] in s:
              c=line.index(k[0])
              if(len(s)>=5 and len(s)<=15):
                  if((c<=(len(s[0])+len(s[1])+len(s[3])+len(s[3]))) or (c>=(len(s)-(len(s[len(s)-1])+len(s[len(s)-2])+len(s[len(s)-3])+len(s[len(s)-4]))))):
                      word_index=0
                      for j in range(len(s)):
                          if(s[j]==k[0]):
                              word_index=j
                      if((word_index!=0) and (word_index!=(len(s)-2)) and (word_index!=(len(s)-1))):
                          if((s[word_index-1] in list_of_preposition_) or (s[word_index+1] in list_of_preposition_)):
                              new_sentence=re.sub(k[0],"_________",line)
                              new_sentence=new_sentence+" .\n"
                              dict_inner={}
                              dict_inner["ques"]=new_sentence.strip("\n")
                              dict_inner["ans"]=k[0]
                              value_list.append(dict_inner)
                              no_of_question +=1
    return (value_list, no_of_question)

if __name__ == "__main__":
    # input from command line
    main(sys.argv[1:])
    create_nouns_list(sys.argv[1])
    create_preposition_list(sys.argv[1])
    # max no of questions in the quiz
    # 30 is a hardcoded value
    max_questions = 30 
    path=""
    if(len(sys.argv)==5):
        glossary_file=sys.argv[4]
        path = os.getcwd() + "\\"+glossary_file
    # case 1
    # generating questions from the summary
    (value_list, no_of_questions) = summary_putting_blanks(sys.argv[1:])

    # case 2
    if(no_of_questions < max_questions and os.path.exists(path)):
        # if the glossary is provided by teacher
        # and if no of question genterated by the previous summary_putting_blanks function is less than 30
        (temp_list, temp_no_of_ques) = find_from_glossary(sys.argv[1],sys.argv[4])
        value_list.extend(temp_list)
        no_of_questions += temp_no_of_ques

    # case 3
    if(no_of_questions < max_questions):
        # if the no of questions is still less than 30
        # then generate questions using the input file itself
        # very crude method
        (temp_list, temp_no_of_ques) = input_putting_blanks(sys.argv[1:])
        no_of_questions += temp_no_of_ques
        value_list.extend(temp_list)

    # case 4 : Error
    if(no_of_questions < max_questions):
        print("Please input a bigger file as there isn't enough text to generate the required number of questions")

    # in json
    dict_outer={}
    dict_outer["questions"]=value_list
    dict_outer = json.dumps(dict_outer)

    # output
    print (dict_outer)

    # write questions in to a file 
    # for debugging only
    # f_out=open("question.txt","w")
    # f_out.write(dict_outer)
    # f_out.close()